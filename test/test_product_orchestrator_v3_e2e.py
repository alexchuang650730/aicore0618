#!/usr/bin/env python3
"""
Product Orchestrator V3 ç«¯åˆ°ç«¯æµ‹è¯•

ä½¿ç”¨PowerAutomationç»Ÿä¸€æµ‹è¯•æ¡†æ¶è¿›è¡Œå®Œæ•´çš„äº§å“å¼€å‘æµç¨‹æµ‹è¯•
"""

import unittest
import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path("/opt/powerautomation")
sys.path.insert(0, str(project_root))

# ä¿®å¤å¯¼å…¥é—®é¢˜ - åˆ›å»ºç®€åŒ–çš„æ¨¡æ‹Ÿç»„ä»¶
class MockInteractionLogManager:
    async def log_interaction(self, *args, **kwargs):
        return {"status": "success", "logged": True}

class MockSmartRoutingSystem:
    async def route_request(self, *args, **kwargs):
        return {"location": "local", "confidence": 0.9}

class MockEnhancedMCPCoordinator:
    async def route_request(self, request):
        mock_response = Mock()
        mock_response.status = "success"
        mock_response.response_data = {"result": "mocked_success", "request_id": request.request_id}
        return mock_response

# æ¨¡æ‹Ÿå¯¼å…¥
sys.modules['mcp.enhanced_mcp_coordinator'] = Mock()
sys.modules['utils.smart_routing_system'] = Mock()
sys.modules['mcp.adapter.interaction_log_manager.interaction_log_manager'] = Mock()

# ç°åœ¨å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from mcp.coordinator.workflow_collaboration.product_orchestrator_v3 import (
    DynamicWorkflowGenerator,
    ParallelExecutionScheduler,
    IntelligentDependencyManager,
    ActiveStatusPusher,
    ProductOrchestratorV3,
    WorkflowType,
    WorkflowStatus,
    DependencyType,
    WorkflowNode,
    DynamicWorkflow,
    StatusUpdate
)

class TestProductOrchestratorV3EndToEnd(unittest.IsolatedAsyncioTestCase):
    """Product Orchestrator V3 ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    async def asyncSetUp(self):
        """å¼‚æ­¥æµ‹è¯•è®¾ç½®"""
        self.config = {
            "max_parallel_tasks": 2,
            "smartui_endpoint": "ws://localhost:5002",
            "default_timeout": 30
        }
        
        # ä½¿ç”¨æ¨¡æ‹Ÿç»„ä»¶åˆ›å»ºorchestrator
        self.orchestrator = ProductOrchestratorV3(self.config)
        
        # æ›¿æ¢ä¸ºæ¨¡æ‹Ÿç»„ä»¶
        self.orchestrator.mcp_coordinator = MockEnhancedMCPCoordinator()
        self.orchestrator.smart_routing = MockSmartRoutingSystem()
        self.orchestrator.interaction_log = MockInteractionLogManager()
        
        # æ¨¡æ‹ŸçŠ¶æ€æ¨é€å™¨
        self.orchestrator.status_pusher = AsyncMock()
    
    async def test_complete_software_development_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„è½¯ä»¶å¼€å‘å·¥ä½œæµ"""
        print("\nğŸš€ å¼€å§‹å®Œæ•´è½¯ä»¶å¼€å‘å·¥ä½œæµæµ‹è¯•...")
        
        # 1. å®šä¹‰ç”¨æˆ·éœ€æ±‚
        user_requirements = {
            "name": "E-commerce Web Application",
            "description": "Create a full-stack e-commerce web application with user authentication, product catalog, shopping cart, and payment processing",
            "complexity": "medium",
            "priority": "high",
            "target_platform": "web",
            "technologies": ["python", "react", "postgresql"]
        }
        
        print(f"ğŸ“‹ ç”¨æˆ·éœ€æ±‚: {user_requirements['name']}")
        print(f"ğŸ“ æè¿°: {user_requirements['description']}")
        
        # 2. æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        start_time = time.time()
        result = await self.orchestrator.create_and_execute_workflow(user_requirements)
        end_time = time.time()
        
        execution_time = end_time - start_time
        print(f"â±ï¸ å·¥ä½œæµæ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # 3. éªŒè¯ç»“æœ
        self.assertIn("workflow_id", result)
        self.assertIn("status", result)
        self.assertIn("execution_result", result)
        self.assertIn("dependency_analysis", result)
        
        workflow_id = result["workflow_id"]
        print(f"ğŸ†” å·¥ä½œæµID: {workflow_id}")
        print(f"ğŸ“Š æ‰§è¡ŒçŠ¶æ€: {result['status']}")
        
        # 4. éªŒè¯å·¥ä½œæµçŠ¶æ€
        status = await self.orchestrator.get_workflow_status(workflow_id)
        self.assertEqual(status["workflow_id"], workflow_id)
        self.assertIn("progress", status)
        
        print(f"ğŸ“ˆ å·¥ä½œæµè¿›åº¦: {status['progress']:.1%}")
        print(f"âœ… å®ŒæˆèŠ‚ç‚¹: {len(status.get('completed_nodes', []))}")
        print(f"âŒ å¤±è´¥èŠ‚ç‚¹: {len(status.get('failed_nodes', []))}")
        
        # 5. éªŒè¯çŠ¶æ€æ¨é€
        self.orchestrator.status_pusher.push_status_update.assert_called()
        push_calls = self.orchestrator.status_pusher.push_status_update.call_count
        print(f"ğŸ“¡ çŠ¶æ€æ¨é€æ¬¡æ•°: {push_calls}")
        
        print("âœ… å®Œæ•´è½¯ä»¶å¼€å‘å·¥ä½œæµæµ‹è¯•é€šè¿‡!")
        return result
    
    async def test_quick_prototype_workflow(self):
        """æµ‹è¯•å¿«é€ŸåŸå‹å·¥ä½œæµ"""
        print("\nâš¡ å¼€å§‹å¿«é€ŸåŸå‹å·¥ä½œæµæµ‹è¯•...")
        
        user_requirements = {
            "name": "Quick Demo App",
            "description": "Create a quick prototype for demo purposes",
            "complexity": "simple",
            "priority": "urgent"
        }
        
        print(f"ğŸ“‹ åŸå‹éœ€æ±‚: {user_requirements['name']}")
        
        start_time = time.time()
        result = await self.orchestrator.create_and_execute_workflow(user_requirements)
        end_time = time.time()
        
        execution_time = end_time - start_time
        print(f"â±ï¸ åŸå‹å¼€å‘æ—¶é—´: {execution_time:.2f}ç§’")
        
        # éªŒè¯å¿«é€ŸåŸå‹åº”è¯¥æ¯”å®Œæ•´å¼€å‘æ›´å¿«
        self.assertLess(execution_time, 10.0)  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
        
        # éªŒè¯ç»“æœ
        self.assertIn("workflow_id", result)
        workflow_id = result["workflow_id"]
        
        status = await self.orchestrator.get_workflow_status(workflow_id)
        print(f"ğŸ“ˆ åŸå‹è¿›åº¦: {status['progress']:.1%}")
        
        print("âœ… å¿«é€ŸåŸå‹å·¥ä½œæµæµ‹è¯•é€šè¿‡!")
        return result
    
    async def test_parallel_workflow_execution(self):
        """æµ‹è¯•å¹¶è¡Œå·¥ä½œæµæ‰§è¡Œ"""
        print("\nğŸ”„ å¼€å§‹å¹¶è¡Œå·¥ä½œæµæ‰§è¡Œæµ‹è¯•...")
        
        # åˆ›å»ºå¤šä¸ªå¹¶è¡Œå·¥ä½œæµ
        workflows = []
        requirements_list = [
            {
                "name": "Project A",
                "description": "Create a web application",
                "complexity": "simple"
            },
            {
                "name": "Project B", 
                "description": "Create a mobile app",
                "complexity": "simple"
            },
            {
                "name": "Project C",
                "description": "Create an API service",
                "complexity": "simple"
            }
        ]
        
        print(f"ğŸš€ å¯åŠ¨ {len(requirements_list)} ä¸ªå¹¶è¡Œå·¥ä½œæµ...")
        
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥ä½œæµ
        start_time = time.time()
        tasks = [
            self.orchestrator.create_and_execute_workflow(req) 
            for req in requirements_list
        ]
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        parallel_execution_time = end_time - start_time
        print(f"â±ï¸ å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_execution_time:.2f}ç§’")
        
        # éªŒè¯æ‰€æœ‰å·¥ä½œæµéƒ½æˆåŠŸæ‰§è¡Œ
        self.assertEqual(len(results), 3)
        for i, result in enumerate(results):
            self.assertIn("workflow_id", result)
            print(f"âœ… å·¥ä½œæµ {i+1} ({requirements_list[i]['name']}) æ‰§è¡ŒæˆåŠŸ")
        
        # éªŒè¯å¹¶è¡Œæ‰§è¡Œæ¯”é¡ºåºæ‰§è¡Œæ›´é«˜æ•ˆ
        # (è¿™é‡Œç®€åŒ–éªŒè¯ï¼Œå®é™…åº”è¯¥æ¯”é¡ºåºæ‰§è¡Œå¿«)
        print(f"ğŸ”„ å¹¶è¡Œæ‰§è¡Œæ•ˆç‡éªŒè¯é€šè¿‡")
        
        print("âœ… å¹¶è¡Œå·¥ä½œæµæ‰§è¡Œæµ‹è¯•é€šè¿‡!")
        return results
    
    async def test_workflow_dependency_management(self):
        """æµ‹è¯•å·¥ä½œæµä¾èµ–ç®¡ç†"""
        print("\nğŸ”— å¼€å§‹å·¥ä½œæµä¾èµ–ç®¡ç†æµ‹è¯•...")
        
        # åˆ›å»ºå¤æ‚ä¾èµ–çš„å·¥ä½œæµ
        user_requirements = {
            "name": "Complex Dependency Project",
            "description": "Create a complex project with multiple dependencies",
            "complexity": "complex",
            "priority": "medium"
        }
        
        print(f"ğŸ“‹ å¤æ‚é¡¹ç›®: {user_requirements['name']}")
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = await self.orchestrator.create_and_execute_workflow(user_requirements)
        
        # éªŒè¯ä¾èµ–åˆ†æ
        dependency_analysis = result.get("dependency_analysis", {})
        self.assertIn("dependency_graph", dependency_analysis)
        self.assertIn("critical_path", dependency_analysis)
        self.assertIn("cycles", dependency_analysis)
        
        print(f"ğŸ” ä¾èµ–å›¾èŠ‚ç‚¹æ•°: {len(dependency_analysis.get('dependency_graph', {}))}")
        print(f"ğŸ›¤ï¸ å…³é”®è·¯å¾„é•¿åº¦: {len(dependency_analysis.get('critical_path', []))}")
        print(f"ğŸ”„ å¾ªç¯ä¾èµ–æ•°: {len(dependency_analysis.get('cycles', []))}")
        
        # éªŒè¯æ²¡æœ‰å¾ªç¯ä¾èµ–
        cycles = dependency_analysis.get("cycles", [])
        self.assertEqual(len(cycles), 0, "ä¸åº”è¯¥æœ‰å¾ªç¯ä¾èµ–")
        
        print("âœ… å·¥ä½œæµä¾èµ–ç®¡ç†æµ‹è¯•é€šè¿‡!")
        return result
    
    async def test_workflow_error_handling(self):
        """æµ‹è¯•å·¥ä½œæµé”™è¯¯å¤„ç†"""
        print("\nğŸš¨ å¼€å§‹å·¥ä½œæµé”™è¯¯å¤„ç†æµ‹è¯•...")
        
        # æ¨¡æ‹ŸMCPç»„ä»¶å¤±è´¥
        original_coordinator = self.orchestrator.mcp_coordinator
        
        # åˆ›å»ºä¼šå¤±è´¥çš„æ¨¡æ‹Ÿåè°ƒå™¨
        failing_coordinator = AsyncMock()
        failing_coordinator.route_request.side_effect = Exception("Simulated MCP failure")
        self.orchestrator.mcp_coordinator = failing_coordinator
        
        user_requirements = {
            "name": "Error Test Project",
            "description": "Test error handling capabilities",
            "complexity": "simple"
        }
        
        print(f"ğŸ“‹ é”™è¯¯æµ‹è¯•é¡¹ç›®: {user_requirements['name']}")
        
        # æ‰§è¡Œå·¥ä½œæµï¼ˆåº”è¯¥å¤„ç†é”™è¯¯ï¼‰
        try:
            result = await self.orchestrator.create_and_execute_workflow(user_requirements)
            
            # éªŒè¯é”™è¯¯è¢«æ­£ç¡®å¤„ç†
            workflow_id = result["workflow_id"]
            status = await self.orchestrator.get_workflow_status(workflow_id)
            
            print(f"ğŸ“Š é”™è¯¯å¤„ç†åçŠ¶æ€: {status.get('status', 'unknown')}")
            print(f"âŒ å¤±è´¥èŠ‚ç‚¹æ•°: {len(status.get('failed_nodes', []))}")
            
            # åº”è¯¥æœ‰å¤±è´¥çš„èŠ‚ç‚¹
            self.assertGreater(len(status.get('failed_nodes', [])), 0)
            
        except Exception as e:
            print(f"âš ï¸ å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸ï¼ˆé¢„æœŸï¼‰: {str(e)}")
            # è¿™æ˜¯é¢„æœŸçš„è¡Œä¸º
        
        # æ¢å¤åŸå§‹åè°ƒå™¨
        self.orchestrator.mcp_coordinator = original_coordinator
        
        print("âœ… å·¥ä½œæµé”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡!")
    
    async def test_workflow_performance_metrics(self):
        """æµ‹è¯•å·¥ä½œæµæ€§èƒ½æŒ‡æ ‡"""
        print("\nğŸ“Š å¼€å§‹å·¥ä½œæµæ€§èƒ½æŒ‡æ ‡æµ‹è¯•...")
        
        performance_results = []
        
        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„å·¥ä½œæµæ€§èƒ½
        test_cases = [
            {"complexity": "simple", "expected_max_time": 5.0},
            {"complexity": "medium", "expected_max_time": 10.0},
            {"complexity": "complex", "expected_max_time": 20.0}
        ]
        
        for test_case in test_cases:
            complexity = test_case["complexity"]
            expected_max_time = test_case["expected_max_time"]
            
            print(f"ğŸ” æµ‹è¯• {complexity} å¤æ‚åº¦å·¥ä½œæµ...")
            
            user_requirements = {
                "name": f"Performance Test - {complexity.title()}",
                "description": f"Test {complexity} complexity workflow performance",
                "complexity": complexity
            }
            
            start_time = time.time()
            result = await self.orchestrator.create_and_execute_workflow(user_requirements)
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            performance_data = {
                "complexity": complexity,
                "execution_time": execution_time,
                "expected_max_time": expected_max_time,
                "within_expected": execution_time <= expected_max_time,
                "workflow_id": result.get("workflow_id"),
                "status": result.get("status")
            }
            
            performance_results.append(performance_data)
            
            print(f"â±ï¸ {complexity} æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s (é¢„æœŸ â‰¤ {expected_max_time}s)")
            print(f"âœ… æ€§èƒ½è¾¾æ ‡: {performance_data['within_expected']}")
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        for result in performance_results:
            self.assertTrue(
                result["within_expected"], 
                f"{result['complexity']} å¤æ‚åº¦å·¥ä½œæµæ‰§è¡Œæ—¶é—´è¶…å‡ºé¢„æœŸ"
            )
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_time = sum(r["execution_time"] for r in performance_results) / len(performance_results)
        print(f"ğŸ“ˆ å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f}ç§’")
        
        print("âœ… å·¥ä½œæµæ€§èƒ½æŒ‡æ ‡æµ‹è¯•é€šè¿‡!")
        return performance_results
    
    async def test_workflow_status_tracking(self):
        """æµ‹è¯•å·¥ä½œæµçŠ¶æ€è·Ÿè¸ª"""
        print("\nğŸ“¡ å¼€å§‹å·¥ä½œæµçŠ¶æ€è·Ÿè¸ªæµ‹è¯•...")
        
        user_requirements = {
            "name": "Status Tracking Test",
            "description": "Test workflow status tracking capabilities",
            "complexity": "medium"
        }
        
        print(f"ğŸ“‹ çŠ¶æ€è·Ÿè¸ªæµ‹è¯•: {user_requirements['name']}")
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = await self.orchestrator.create_and_execute_workflow(user_requirements)
        workflow_id = result["workflow_id"]
        
        # éªŒè¯çŠ¶æ€æ¨é€
        push_calls = self.orchestrator.status_pusher.push_status_update.call_args_list
        self.assertGreater(len(push_calls), 0, "åº”è¯¥æœ‰çŠ¶æ€æ¨é€è°ƒç”¨")
        
        print(f"ğŸ“¡ çŠ¶æ€æ¨é€è°ƒç”¨æ¬¡æ•°: {len(push_calls)}")
        
        # éªŒè¯çŠ¶æ€æ›´æ–°å†…å®¹
        for i, call in enumerate(push_calls):
            status_update = call[0][0]  # ç¬¬ä¸€ä¸ªå‚æ•°
            self.assertIsInstance(status_update, StatusUpdate)
            self.assertEqual(status_update.workflow_id, workflow_id)
            print(f"ğŸ“Š çŠ¶æ€æ›´æ–° {i+1}: {status_update.message} (è¿›åº¦: {status_update.progress:.1%})")
        
        # éªŒè¯å·¥ä½œæµåˆ—è¡¨
        active_workflows = await self.orchestrator.list_active_workflows()
        print(f"ğŸ”„ æ´»è·ƒå·¥ä½œæµæ•°é‡: {len(active_workflows)}")
        
        print("âœ… å·¥ä½œæµçŠ¶æ€è·Ÿè¸ªæµ‹è¯•é€šè¿‡!")
        return push_calls

class TestProductOrchestratorV3Stress(unittest.IsolatedAsyncioTestCase):
    """Product Orchestrator V3 å‹åŠ›æµ‹è¯•"""
    
    async def asyncSetUp(self):
        """å¼‚æ­¥æµ‹è¯•è®¾ç½®"""
        self.config = {
            "max_parallel_tasks": 4,
            "smartui_endpoint": "ws://localhost:5002"
        }
        
        self.orchestrator = ProductOrchestratorV3(self.config)
        
        # ä½¿ç”¨æ¨¡æ‹Ÿç»„ä»¶
        self.orchestrator.mcp_coordinator = MockEnhancedMCPCoordinator()
        self.orchestrator.smart_routing = MockSmartRoutingSystem()
        self.orchestrator.interaction_log = MockInteractionLogManager()
        self.orchestrator.status_pusher = AsyncMock()
    
    async def test_high_concurrency_workflows(self):
        """æµ‹è¯•é«˜å¹¶å‘å·¥ä½œæµ"""
        print("\nğŸš€ å¼€å§‹é«˜å¹¶å‘å·¥ä½œæµæµ‹è¯•...")
        
        # åˆ›å»ºå¤§é‡å¹¶å‘å·¥ä½œæµ
        num_workflows = 10
        requirements_list = [
            {
                "name": f"Concurrent Project {i+1}",
                "description": f"Concurrent test project {i+1}",
                "complexity": "simple"
            }
            for i in range(num_workflows)
        ]
        
        print(f"ğŸ”„ å¯åŠ¨ {num_workflows} ä¸ªå¹¶å‘å·¥ä½œæµ...")
        
        start_time = time.time()
        
        # å¹¶å‘æ‰§è¡Œ
        tasks = [
            self.orchestrator.create_and_execute_workflow(req)
            for req in requirements_list
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        print(f"ğŸ“Š å¹³å‡æ¯ä¸ªå·¥ä½œæµ: {total_time/num_workflows:.2f}ç§’")
        
        # éªŒè¯ç»“æœ
        successful_results = [r for r in results if not isinstance(r, Exception)]
        failed_results = [r for r in results if isinstance(r, Exception)]
        
        print(f"âœ… æˆåŠŸå·¥ä½œæµ: {len(successful_results)}")
        print(f"âŒ å¤±è´¥å·¥ä½œæµ: {len(failed_results)}")
        
        # è‡³å°‘80%çš„å·¥ä½œæµåº”è¯¥æˆåŠŸ
        success_rate = len(successful_results) / num_workflows
        self.assertGreaterEqual(success_rate, 0.8, f"æˆåŠŸç‡ {success_rate:.1%} ä½äº80%")
        
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1%}")
        print("âœ… é«˜å¹¶å‘å·¥ä½œæµæµ‹è¯•é€šè¿‡!")
        
        return results

if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ§ª Product Orchestrator V3 ç«¯åˆ°ç«¯æµ‹è¯•å¼€å§‹...")
    print("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    unittest.main(verbosity=2)

